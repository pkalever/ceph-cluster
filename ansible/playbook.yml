---
##############################
# This playbook is called by Vagrant for initial VM provisioning.
# Bringup the ceph environment

- hosts: all
  gather_facts: true
  become: true
  any_errors_fatal: true
  roles:
    - common

- hosts: ceph_admin[0]
  gather_facts: false
  become: true
  become_user: vagrant
  become_method: sudo
  any_errors_fatal: true
  roles:
    - ceph-admin

- hosts: ceph_nodes
  gather_facts: false
  become: true
  any_errors_fatal: true
  tasks:
    - name: Create mount directories for rbd and cephfs
      file:
        path: "{{ item }}"
        state: directory
      with_items:
        - "/mnt/rbd"
        - "/mnt/cephfs"

- hosts: ceph_nodes[0]
  gather_facts: false
  become: true
  any_errors_fatal: true
  tasks:
    - name: Create ceph rbd pool
      command: "{{ item }}"
      with_items:
        - ceph osd pool create rbd-pool 64
        - rbd pool init rbd-pool
    - name: Create ceph file system
      command: "{{ item }}"
      with_items:
        - ceph osd pool create cephfs-datapool 64
        - ceph osd pool create cephfs-metapool 32
        - ceph fs new cephfs cephfs-metapool cephfs-datapool
    - name: Creating an image file in rbd block pool and map
      command: "{{ item }}"
      with_items:
        - rbd create --size 1024 rbd-pool/image1 --image-feature layering
    - name: Check rbd image info
      command: rbd info rbd-pool/image1
      register: rbd_info
    - debug:
        msg:
          - "# rbd info rbd-pool/image1"
          - "{{ rbd_info.stdout_lines }}"
    - name: Check ceph fs status
      command: ceph fs status
      register: cephfs_status
    - debug:
        msg:
          - "# ceph fs status"
          - "{{ cephfs_status.stdout_lines }}"
    - name: Check ceph health status
      command: ceph --status
      register: status
    - debug:
        msg:
          - "# ceph --status"
          - "{{ status.stdout_lines }}"
    - name: Get cluster ID
      command: "ceph fsid"
      register: fsid
    - name: Get mon ips
      shell: "ceph mon metadata | grep addrs | cut -d':' -f3"
      register: mon_ips
    - name: Get the secret
      command: "ceph-authtool -p /etc/ceph/ceph.client.admin.keyring"
      register: secret
    - debug:
        msg:
          - "--------------- CEPH CLUSTER SUMMARY  ------------"
          - "❇ Cluster ID: "
          - "    {{ fsid.stdout_lines[0] }} "
          - "❇ Mon IPs: "
          - "    {{ mon_ips.stdout_lines }} "
          - "❇ Admin Key:"
          - "    {{ secret.stdout_lines[0] }} "
          - ""
          - "❇ Rbd pool name: "
          - "    rbd-pool "
          - "❇ To mount the precreated rbd image, run: "
          - "    # vagrant ssh <{{ groups['ceph_nodes'] | join('|') }}> "
          - "    # sudo rbd device map rbd-pool/image1 "
          - "    # sudo mkfs.ext4 /dev/rbd0 "
          - "    # sudo mount /dev/rbd0 /mnt/rbd "
          - ""
          - "❇ Ceph FS name: "
          - "    cephfs "
          - "❇ To mount the ceph fs, run:"
          - "    # vagrant ssh <{{ groups['ceph_nodes'] | join('|') }}> "
          - "    # sudo mount -t ceph {{ groups['ceph_nodes'][0] }}:6789:/ /mnt/cephfs -o name=admin,secret='{{ secret.stdout_lines[0] }}' "
          - "----------------------------------------------------"
